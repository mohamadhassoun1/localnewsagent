LocalNewsAgent Project Structure
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

blog/
â”‚
â”œâ”€â”€ ğŸ“„ Core Application (Ready to Run!)
â”‚   â”œâ”€â”€ main.py                                 â­ MAIN ENTRY POINT
â”‚   â”‚   â””â”€ Run with: python main.py
â”‚   â”‚
â”‚   â””â”€â”€ src/                                    ğŸ§© MODULES
â”‚       â”œâ”€â”€ __init__.py                         (package marker)
â”‚       â”œâ”€â”€ discovery.py                        (A. Topic discovery - RSS/SERP)
â”‚       â”œâ”€â”€ extractor.py                        (B. Content extraction - facts/bullets)
â”‚       â”œâ”€â”€ composer.py                         (C. Article generation - LLM/fallback)
â”‚       â”œâ”€â”€ qa.py                               (E. Quality assurance - validation)
â”‚       â””â”€â”€ output.py                           (F. Output - JSON/HTML/logs)
â”‚
â”œâ”€â”€ âš™ï¸ Configuration Files (Edit These!)
â”‚   â”œâ”€â”€ rss_feeds.txt                           ğŸ“‹ RSS feed URLs (one per line)
â”‚   â”œâ”€â”€ sites.txt                               ğŸŒ Base URLs to crawl (one per line)
â”‚   â””â”€â”€ requirements.txt                        ğŸ“¦ Python dependencies
â”‚
â”œâ”€â”€ ğŸ“– Documentation (Read These!)
â”‚   â”œâ”€â”€ README.md                               ğŸ“š Full system documentation
â”‚   â”œâ”€â”€ QUICKSTART.md                           âš¡ 5-minute setup guide
â”‚   â”œâ”€â”€ ARCHITECTURE.md                         ğŸ—ï¸ Technical deep-dive
â”‚   â”œâ”€â”€ CONFIG_GUIDE.md                         âš™ï¸ Configuration reference
â”‚   â”œâ”€â”€ GETTING_STARTED.txt                     ğŸ‘‹ First-time setup guide
â”‚   â””â”€â”€ DEPLOYMENT_REPORT.txt                   âœ… This deployment summary
â”‚
â”œâ”€â”€ ğŸ“ Output Directories (Auto-Created)
â”‚   â”œâ”€â”€ drafts/                                 ğŸ“„ Publish-ready articles
â”‚   â”‚   â”œâ”€â”€ {slug}__{timestamp}.json            JSON with metadata + HTML
â”‚   â”‚   â”œâ”€â”€ {slug}.html                         Standalone HTML page
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
â”‚   â”œâ”€â”€ logs/                                   ğŸ“ Session & activity logs
â”‚   â”‚   â”œâ”€â”€ session_{timestamp}.log             Full workflow transcript
â”‚   â”‚   â”œâ”€â”€ {slug}.log                          Per-article details
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
â”‚   â””â”€â”€ published/                              âœ”ï¸ Published articles (manual)
â”‚       â”œâ”€â”€ {slug}__{timestamp}_published.json
â”‚       â””â”€â”€ ...
â”‚
â””â”€â”€ .gitignore (optional)
    â””â”€ For version control (drafts/, logs/, published/)


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
USAGE FLOW
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. SETUP
   â”œâ”€ pip install -r requirements.txt
   â””â”€ (optional) pip install gpt4all

2. CONFIGURE
   â”œâ”€ Edit rss_feeds.txt (add RSS feed URLs)
   â””â”€ Edit sites.txt (add base URLs to crawl)

3. RUN
   â”œâ”€ python main.py              # Default (3 topics, 800+ words)
   â”œâ”€ python main.py --top-topics 5        # 5 topics
   â”œâ”€ python main.py --min-words 1000      # 1000+ words
   â”œâ”€ python main.py --no-llm              # Skip LLM, use fallback
   â””â”€ python main.py --queries "AI" "blockchain"

4. OUTPUTS
   â”œâ”€ Check ./drafts/ for JSON + HTML
   â”œâ”€ Check ./logs/ for session details
   â””â”€ Review article quality

5. PUBLISH
   â”œâ”€ Review drafts in ./drafts/
   â”œâ”€ Edit as needed
   â”œâ”€ Generate images from prompts
   â”œâ”€ Post to CMS
   â””â”€ Move to ./published/ (optional)


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
WORKFLOW (A â†’ F)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

A. DISCOVERY (discovery.py)
   â”œâ”€ Load RSS feeds from rss_feeds.txt
   â”œâ”€ Discover /rss /feed endpoints from sites.txt
   â”œâ”€ (Optional) Scrape DuckDuckGo for queries
   â””â”€ Output: List[Topic] sorted by trend_score

B. EXTRACTION (extractor.py)
   â”œâ”€ Fetch article URLs
   â”œâ”€ Detect paywalls (skip if found)
   â”œâ”€ Extract 5-12 bullet facts (â‰¤25 words each)
   â”œâ”€ Rewrite excerpts (â‰¤50 words)
   â””â”€ Output: List[ExtractedArticle]

C. COMPOSITION (composer.py)
   â”œâ”€ Try local LLM (gpt4all) if available
   â”œâ”€ Fallback to rule-based generation
   â”œâ”€ Generate 800-1600 word article
   â”œâ”€ Create HTML with structure
   â”œâ”€ Generate 3 image prompts (16:9)
   â””â”€ Output: ComposedArticle (with all metadata)

D. SEO & IMAGES (integrated in C)
   â”œâ”€ SEO Title (â‰¤60 chars)
   â”œâ”€ Meta Description (120-155 chars)
   â”œâ”€ Keywords (8-12 items)
   â”œâ”€ Tags (5-8 items)
   â”œâ”€ Image Prompts (3 variations + alt_text)
   â””â”€ Internal links (/tag/*, /category/*)

E. QA (qa.py)
   â”œâ”€ Word count >= minimum (default: 800)
   â”œâ”€ Plagiarism check (20-char threshold)
   â”œâ”€ AdSense safety (block disallowed content)
   â”œâ”€ Structural validation (Our take + Sources)
   â””â”€ Auto-regenerate if fail (up to 2x)

F. OUTPUT (output.py)
   â”œâ”€ Save JSON draft to ./drafts/
   â”œâ”€ Save HTML to ./drafts/
   â”œâ”€ Create session log to ./logs/
   â””â”€ Display results


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
FILE PURPOSES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

main.py (15 KB)
  â€¢ CLI entry point
  â€¢ Orchestrates Aâ†’F workflow
  â€¢ Accepts command-line arguments
  â€¢ Logs entire session
  â€¢ Provides user feedback

src/discovery.py (12 KB)
  â€¢ Fetches RSS feeds
  â€¢ Discovers feed URLs from sites
  â€¢ Scrapes DuckDuckGo results (no API)
  â€¢ Builds topic list with trend scores
  â€¢ Exports dataclass: Topic

src/extractor.py (8 KB)
  â€¢ Fetches article content
  â€¢ Detects paywalls
  â€¢ Extracts facts and links
  â€¢ Rewrites excerpts
  â€¢ Exports dataclass: ExtractedArticle

src/composer.py (18 KB)
  â€¢ Interfaces with local LLM (gpt4all/Ollama)
  â€¢ Falls back to rule-based generation
  â€¢ Generates article HTML
  â€¢ Creates SEO metadata
  â€¢ Generates image prompts
  â€¢ Exports dataclass: ComposedArticle

src/qa.py (6 KB)
  â€¢ Checks word count
  â€¢ Detects plagiarism (20-char threshold)
  â€¢ Filters AdSense-unsafe content
  â€¢ Validates structure
  â€¢ Exports dataclass: QAResult

src/output.py (10 KB)
  â€¢ Saves JSON drafts
  â€¢ Generates HTML pages
  â€¢ Creates session logs
  â€¢ Manages file I/O
  â€¢ Lists drafts/published

rss_feeds.txt (0.4 KB)
  â€¢ RSS feed URLs (one per line)
  â€¢ Pre-configured examples
  â€¢ Add your own feeds here

sites.txt (0.3 KB)
  â€¢ Base URLs to crawl
  â€¢ Agent discovers /rss /feed endpoints
  â€¢ Pre-configured examples
  â€¢ Add your own sites here

requirements.txt (0.5 KB)
  â€¢ Core: feedparser, requests, beautifulsoup4, lxml
  â€¢ Optional: gpt4all (local LLM)
  â€¢ Optional: playwright (dynamic content)


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
JSON OUTPUT FORMAT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Location: ./drafts/{slug}__{timestamp}.json

Fields:
  title                    Article headline (â‰¤100 chars)
  article_html             Full HTML with <h2>, <p>, <ul>/<li>
  summary                  40-60 word summary
  word_count               Total words in article
  tags                     5-8 category tags
  slug                     URL-friendly slug
  seo_title                â‰¤60 chars for search results
  meta_description         120-155 chars for meta tag
  keywords                 8-12 SEO keywords
  image_prompt_main        Primary 16:9 image prompt
  image_prompt_alt1        Alternative image prompt
  image_prompt_alt2        Another alternative prompt
  alt_text                 â‰¤120 chars for accessibility
  sources                  [{name, url}, ...]
  cta                      Call-to-action text
  our_take                 1-sentence analysis
  published_at             ISO timestamp


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
LOG FILE FORMAT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Location: ./logs/{slug}.log

Contains:
  DISCOVERY PHASE         - Topics found, sources processed
  EXTRACTION PHASE        - Facts extracted, paywalls skipped
  COMPOSITION PHASE       - LLM used or fallback method
  QA & VALIDATION PHASE   - Validation results, issues if any
  OUTPUT PHASE            - Files saved, locations
  SOURCES USED            - All source URLs with names


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
COMMAND REFERENCE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Default (3 articles):
  python main.py

Custom topic count:
  python main.py --top-topics 5

Minimum word count:
  python main.py --min-words 1200

Skip LLM (faster):
  python main.py --no-llm

Custom searches:
  python main.py --queries "AI news" "blockchain"

Everything combined:
  python main.py --top-topics 5 --min-words 1000 --no-llm --queries "AI" "tech"

Custom config files:
  python main.py --rss-file my_feeds.txt --sites-file my_sites.txt


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
QUICK START
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Install:
   pip install -r requirements.txt

2. Run:
   python main.py

3. Check output:
   ls drafts/

4. View article:
   cat drafts/article-slug.html


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

For detailed information, see: README.md
For setup instructions, see: QUICKSTART.md
For technical details, see: ARCHITECTURE.md
For configuration help, see: CONFIG_GUIDE.md
